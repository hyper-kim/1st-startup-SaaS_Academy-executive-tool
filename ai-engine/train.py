# ai-engine/train.py

import os
import json
import torch
from PIL import Image
from torch.utils.data import Dataset
from transformers import VisionEncoderDecoderModel, DonutProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer

# ---------------------------------------------------------
# 1. ì„¤ì • (Config)
# ---------------------------------------------------------
# ëª¨ë¸ ì´ë¦„ (Hugging Face Hubì— ìˆëŠ” ê¸°ë³¸ ëª¨ë¸)
MODEL_ID = "naver-clova-ix/donut-base"

# ë°ì´í„° ê²½ë¡œ (ì•„ê¹Œ ë§Œë“  ë°ì´í„°ì…‹)
DATASET_PATH = "dataset/multi_receipt_train" # (generate_dataset.py ê²°ê³¼ë¬¼ ê²½ë¡œ)
IMAGE_DIR = os.path.join(DATASET_PATH, "images")
LABEL_DIR = os.path.join(DATASET_PATH, "labels")

# í•™ìŠµ ì„¤ì • (RTX 4060 8GB ê¸°ì¤€)
BATCH_SIZE = 1          # VRAM ë¶€ì¡±í•˜ë©´ 1ë¡œ ì¤„ì´ì„¸ìš”
GRADIENT_ACCUMULATION = 8 # 2 * 4 = 8 ë°°ì¹˜ íš¨ê³¼
EPOCHS = 5              # ë°ì´í„°ê°€ ë§ìœ¼ë©´ 3~5ë²ˆë§Œ ë´ë„ ì¶©ë¶„í•¨
LEARNING_RATE = 1e-5

# ---------------------------------------------------------
# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ (Dataset)
# ---------------------------------------------------------
class ReceiptDataset(Dataset):
    def __init__(self, image_dir, label_dir, processor, max_length=768):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.processor = processor
        self.max_length = max_length
        
        # íŒŒì¼ ëª©ë¡ ë¡œë“œ
        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(".jpg")])
        
        # í”„ë¡¬í”„íŠ¸ (ëª¨ë¸ì—ê²Œ "ì´ê±° ì½ì–´ì¤˜"ë¼ê³  ì‹œí‚¤ëŠ” ì‹œì‘ í† í°)
        self.task_prompt = "<s_receipt>"

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # 1. ì´ë¯¸ì§€ ë¡œë“œ
        img_name = self.image_files[idx]
        image_path = os.path.join(self.image_dir, img_name)
        image = Image.open(image_path).convert("RGB")
        
        # 2. ì •ë‹µ ë¼ë²¨(JSON) ë¡œë“œ
        label_name = img_name.replace(".jpg", ".json")
        label_path = os.path.join(self.label_dir, label_name)
        
        with open(label_path, "r", encoding="utf-8") as f:
            label_data = json.load(f)

        # ğŸ’¡ [ìˆ˜ì •] í•™ìŠµ ë°©í•´ ìš”ì†Œì¸ 'file' í‚¤ ì œê±° (ì¤‘ìš”!)
        if "file" in label_data:
            del label_data["file"]
            
        # ëª¨ë¸ì€ ì´ì œ ì˜¤ì§ ì˜ìˆ˜ì¦ ë‚´ìš©({"receipts": [...]})ë§Œ ë°°ì›ë‹ˆë‹¤.
        target_sequence = json.dumps(label_data, ensure_ascii=False)
        
        # 3. ì…ë ¥(Pixel Values) ë³€í™˜
        pixel_values = self.processor(image, return_tensors="pt").pixel_values
        
        # 4. ì •ë‹µ(Labels) í† í°í™”
        input_sequence = self.task_prompt + target_sequence + self.processor.tokenizer.eos_token
        
        labels = self.processor.tokenizer(
            input_sequence,
            add_special_tokens=False,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )["input_ids"]
        
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        return {
            "pixel_values": pixel_values.squeeze(),
            "labels": labels.squeeze()
        }
# ---------------------------------------------------------
# 3. í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
# ---------------------------------------------------------
def train():
    print("ğŸ”¥ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì¸í„°ë„· ì—°ê²° í•„ìš”)")
    # 1. í”„ë¡œì„¸ì„œ(ì´ë¯¸ì§€ ì²˜ë¦¬ê¸° + í† í¬ë‚˜ì´ì €) ë¡œë“œ
    processor = DonutProcessor.from_pretrained(MODEL_ID)
    
    # ëª¨ë¸ì— ìƒˆë¡œìš´ íŠ¹ìˆ˜ í† í°(í•œê¸€ ë“±) ì¶”ê°€
    # (Donut ê¸°ë³¸ ëª¨ë¸ì€ í•œê¸€ì„ ì˜ ì•Œì§€ë§Œ, receipt ê´€ë ¨ íƒœê·¸ë¥¼ ì¶”ê°€í•´ì¤Œ)
    processor.tokenizer.add_tokens(["<s_receipt>", "</s_receipt>"])

    # 2. ëª¨ë¸ ë¡œë“œ
    model = VisionEncoderDecoderModel.from_pretrained(MODEL_ID)
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids("<s_receipt>")
    
    # í† í¬ë‚˜ì´ì € í¬ê¸°ê°€ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì ˆ
    model.decoder.resize_token_embeddings(len(processor.tokenizer))
    
    # GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"âœ… í•™ìŠµ ì¥ì¹˜: {device} (GPU: {torch.cuda.get_device_name(0) if device=='cuda' else 'None'})")

    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = ReceiptDataset(IMAGE_DIR, LABEL_DIR, processor)
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dataset)}ì¥")

    # 4. í•™ìŠµ ì¸ì ì„¤ì •
    training_args = Seq2SeqTrainingArguments(
        output_dir="./result",       # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        fp16=True,                   # GPU ë©”ëª¨ë¦¬ ì ˆì•½ (Mixed Precision)

        gradient_checkpointing = True,

        logging_steps=10,
        save_total_limit=2,          # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ 2ê°œë§Œ ì €ì¥
        remove_unused_columns=False,
        report_to="none",            # WandB ë“± ë„ê¸°
        dataloader_num_workers=0     # ìœˆë„ìš°ì—ì„œëŠ” 0 ê¶Œì¥ (ì—ëŸ¬ ë°©ì§€)
    )

    # 5. íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
    )

    # 6. í•™ìŠµ ì‹œì‘!
    print("ğŸš€ í•™ìŠµ ì‹œì‘! (ì»¤í”¼ í•œ ì” í•˜ê³  ì˜¤ì„¸ìš”)")
    trainer.train()

    # 7. ëª¨ë¸ ì €ì¥
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    save_path = "./models/receipt_model_v1"
    model.save_pretrained(save_path)
    processor.save_pretrained(save_path)
    print(f"ğŸ‰ ì™„ë£Œ! ëª¨ë¸ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # ë°ì´í„°ì…‹ í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
    if not os.path.exists(IMAGE_DIR):
        print(f"âŒ ì˜¤ë¥˜: {IMAGE_DIR} í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. generate_dataset.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        train()