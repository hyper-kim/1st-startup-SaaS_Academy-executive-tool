import os
import json
import glob
import shutil
import zipfile
import torch
from PIL import Image
from torch.utils.data import Dataset
from transformers import (
    VisionEncoderDecoderModel,
    DonutProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    default_data_collator
)

# ==============================================================================
# 1. ì„¤ì • (Configuration)
# ==============================================================================
# â˜… Hugging Face ì €ì¥ì†Œ ID (ë³¸ì¸ ê²ƒìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”!)
HUB_MODEL_ID = "HYPER-KJY/academy-receipt-model"

# ëª¨ë¸ ë² ì´ìŠ¤ (ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë°°ì›€)
MODEL_ID = "naver-clova-ix/donut-base"

# í•™ìŠµ ì„¤ì • (T4 GPU 2ê°œ ê¸°ì¤€ ìµœì í™”)
# ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ BATCH_SIZEë¥¼ 1ë¡œ ì¤„ì´ì„¸ìš”.
BATCH_SIZE = 2
GRADIENT_ACCUMULATION = 4
EPOCHS = 10 # ì¶©ë¶„íˆ í•™ìŠµ
LEARNING_RATE = 2e-5
IMAGE_SIZE = (960, 720) # (Height, Width) - í•´ìƒë„ ê³ ì •

# ê²½ë¡œ ì„¤ì • (Kaggle í™˜ê²½)
WORKING_DIR = "/kaggle/working"
DATASET_DIR = "/kaggle/input/academy-dataset-with-handwriting/dataset/multi_receipt_train"
IMAGE_DIR = f"{DATASET_DIR}/images"
LABEL_DIR = f"{DATASET_DIR}/labels"

# ==============================================================================
# 2. ë°ì´í„° ì¤€ë¹„ (ì••ì¶• í•´ì œ)
# ==============================================================================
def prepare_data():
    if os.path.exists(IMAGE_DIR):
        print(f"âœ… ë°ì´í„°ê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {len(os.listdir(IMAGE_DIR))}ì¥")
        return

    print("ğŸ” Input ë°ì´í„°ì…‹ì—ì„œ ì••ì¶• íŒŒì¼ ì°¾ëŠ” ì¤‘...")
    zip_path = None
    # Kaggle Input í´ë” ë’¤ì§€ê¸°
    for root, dirs, files in os.walk('/kaggle/input'):
        for file in files:
            if file.endswith('.zip'):
                zip_path = os.path.join(root, file)
                break
        if zip_path: break
    
    if not zip_path:
        # í˜¹ì‹œ ë¡œì»¬ì— ìˆë‚˜ í™•ì¸
        if os.path.exists("dataset.zip"): zip_path = "dataset.zip"
        elif os.path.exists("academy_data.zip"): zip_path = "academy_data.zip"

    if zip_path:
        print(f"ğŸ“¦ ì••ì¶• í•´ì œ ì‹œì‘: {zip_path}")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(WORKING_DIR)
        print("âœ… ì••ì¶• í•´ì œ ì™„ë£Œ!")
    else:
        raise FileNotFoundError("âŒ ë°ì´í„°ì…‹ zip íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! [Add Input]ì„ í™•ì¸í•˜ì„¸ìš”.")

# ==============================================================================
# 3. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì „ì²˜ë¦¬ í•µì‹¬)
# ==============================================================================
class ReceiptDataset(Dataset):
    def __init__(self, image_dir, label_dir, processor, max_length=768):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.processor = processor
        self.max_length = max_length
        self.image_files = sorted(glob.glob(os.path.join(image_dir, "*.jpg")))
        self.task_prompt = "<s_receipt>"

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        image = Image.open(image_path).convert("RGB")
        
        # ë¼ë²¨ íŒŒì¼ ì°¾ê¸°
        filename = os.path.basename(image_path)
        label_name = filename.replace(".jpg", ".json")
        label_path = os.path.join(self.label_dir, label_name)
        
        with open(label_path, "r", encoding="utf-8") as f:
            label_data = json.load(f)

        # ------------------------------------------------------------------
        # â˜… [í•µì‹¬ì „ëµ] ë¶ˆí•„ìš”í•œ ì •ë³´ ì‚­ì œ (AI ë‡Œ ìš©ëŸ‰ í™•ë³´)
        # ------------------------------------------------------------------
        if "file" in label_data: del label_data["file"]
        
        if "receipts" in label_data:
            for receipt in label_data["receipts"]:
                # 1. ì¢Œí‘œ ì‚­ì œ (ì´ë¯¸ì§€ ì¸ì‹ ëª¨ë¸ì—ê²Œ ì¢Œí‘œ ì˜ˆì¸¡ì€ ë„ˆë¬´ ì–´ë ¤ì›€)
                if "position" in receipt: del receipt["position"]
                
                # 2. í’ˆëª©(items) ì‚­ì œ (ì´ê²Œ ì œì¼ ì¤‘ìš”! ì´ë¦„/ê¸ˆì•¡ ì •í™•ë„ ê¸‰ìƒìŠ¹ ë¹„ê²°)
                # í…€í”„ ë³´ê³ ì„œìš©: "ë³µì¡ë„ë¥¼ ì¤„ì—¬ í•µì‹¬ ì •ë³´(ê¸ˆì•¡,ì´ë¦„) ì¸ì‹ë¥ ì„ ë†’ì´ëŠ” Feature Selection ìˆ˜í–‰"
                if "items" in receipt: del receipt["items"]
            
        target_sequence = json.dumps(label_data, ensure_ascii=False)
        
        # ì…ë ¥ ì²˜ë¦¬
        pixel_values = self.processor(image, return_tensors="pt").pixel_values
        
        input_sequence = self.task_prompt + target_sequence + self.processor.tokenizer.eos_token
        
        labels = self.processor.tokenizer(
            input_sequence,
            add_special_tokens=False,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )["input_ids"]
        
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        return {
            "pixel_values": pixel_values.squeeze(),
            "labels": labels.squeeze()
        }

# ==============================================================================
# 4. í•™ìŠµ ì‹¤í–‰
# ==============================================================================
def train():
    # 1. ë°ì´í„° ì¤€ë¹„
    prepare_data()

    print("ğŸ”¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    processor = DonutProcessor.from_pretrained(MODEL_ID)
    processor.tokenizer.add_tokens(["<s_receipt>", "</s_receipt>"])
    
    # í•´ìƒë„ ê°•ì œ ê³ ì • (í•™ìŠµ/ì¶”ë¡  ì¼ì¹˜ í•„ìˆ˜)
    processor.image_processor.size = {"height": IMAGE_SIZE[0], "width": IMAGE_SIZE[1]}
    print(f"ğŸ“‰ ì´ë¯¸ì§€ í¬ê¸° ì„¤ì •: {processor.image_processor.size}")

    model = VisionEncoderDecoderModel.from_pretrained(MODEL_ID)
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids("<s_receipt>")
    
    # í•´ìƒë„ ì„¤ì • ëª¨ë¸ì—ë„ ë°˜ì˜
    model.config.encoder.image_size = [IMAGE_SIZE[0], IMAGE_SIZE[1]]
    
    # í† í° ì¶”ê°€í–ˆìœ¼ë‹ˆ ì„ë² ë”© í¬ê¸° ì¡°ì ˆ
    model.decoder.resize_token_embeddings(len(processor.tokenizer))
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"âœ… í•™ìŠµ ì¥ì¹˜: {device}")

    # ë°ì´í„°ì…‹ ì—°ê²°
    train_dataset = ReceiptDataset(IMAGE_DIR, LABEL_DIR, processor)
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dataset)}ì¥")

    # í•™ìŠµ ì¸ì ì„¤ì •
    training_args = Seq2SeqTrainingArguments(
        output_dir="./result",
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        fp16=True,                  # ì†ë„ í–¥ìƒ
        gradient_checkpointing=True, # ë©”ëª¨ë¦¬ ì ˆì•½
        logging_steps=50,
        save_strategy="epoch",
        save_total_limit=2,
        remove_unused_columns=False,
        report_to="none",
        dataloader_num_workers=4,
        
        # Hub ì—…ë¡œë“œ ì„¤ì •
        push_to_hub=True,
        hub_model_id=HUB_MODEL_ID,
        hub_private_repo=True,
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µí‹°ë§ˆì´ì € (bitsandbytes í•„ìš”)
        optim="adamw_bnb_8bit" 
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=processor.tokenizer,
        data_collator=default_data_collator,
    )

    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    trainer.train()

    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ ì¤‘...")
    processor.save_pretrained("./result")
    
    # optimizer.pt ê°™ì€ ê±°ëŒ€ íŒŒì¼ ì œì™¸í•˜ê³  ì—…ë¡œë“œ (ì†ë„/ì—ëŸ¬ ë°©ì§€)
    trainer.push_to_hub(commit_message="Training complete", blocking=True)
    print("ğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ!")

if __name__ == "__main__":
    train()