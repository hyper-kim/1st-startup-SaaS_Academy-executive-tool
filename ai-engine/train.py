import os
import json
import torch
from PIL import Image
from torch.utils.data import Dataset
from transformers import (
    VisionEncoderDecoderModel,
    DonutProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer
)

# ---------------------------------------------------------
# â˜… [ì„¤ì •] Hugging Face Hub ì„¤ì • (ë³¸ì¸ IDë¡œ ìˆ˜ì • í•„ìˆ˜)
# ---------------------------------------------------------
HUB_MODEL_ID = "HYPER-KJY/academy-receipt-model"  # ì˜ˆ: "ë³¸ì¸ì•„ì´ë””/ëª¨ë¸ëª…"
PUSH_TO_HUB = True  # Trueë¡œ ì„¤ì •í•˜ë©´ í•™ìŠµ í›„ ìë™ ì—…ë¡œë“œ

# ëª¨ë¸ ì´ë¦„ (ë² ì´ìŠ¤ ëª¨ë¸)
MODEL_ID = "naver-clova-ix/donut-base"

# ë°ì´í„° ê²½ë¡œ (Repo êµ¬ì¡° ê¸°ì¤€)
# Kaggleì—ì„œ ì‹¤í–‰ ì‹œ !cp -r ... ./dataset ëª…ë ¹ì–´ë¡œ ì´ ìœ„ì¹˜ì— ë°ì´í„°ë¥¼ ë‘ê²Œ ë©ë‹ˆë‹¤.
DATASET_PATH = "dataset/multi_receipt_train"
IMAGE_DIR = os.path.join(DATASET_PATH, "images")
LABEL_DIR = os.path.join(DATASET_PATH, "labels")

# í•™ìŠµ ì„¤ì • (Kaggle P100 GPU ê¸°ì¤€)
# P100ì€ ë©”ëª¨ë¦¬ê°€ ë„‰ë„‰í•˜ë¯€ë¡œ(16GB) ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¡°ê¸ˆ ëŠ˜ë ¤ë„ ë©ë‹ˆë‹¤.
BATCH_SIZE = 2          # 1 -> 2 (í„°ì§€ë©´ 1ë¡œ ì¤„ì´ì„¸ìš”)
GRADIENT_ACCUMULATION = 4 
EPOCHS = 30             # ì¶©ë¶„í•œ í•™ìŠµì„ ìœ„í•´ 30íšŒ ì¶”ì²œ (Donutì€ ì˜¤ë˜ ê±¸ë¦¼)
LEARNING_RATE = 1e-5

# ---------------------------------------------------------
# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ---------------------------------------------------------
class ReceiptDataset(Dataset):
    def __init__(self, image_dir, label_dir, processor, max_length=768):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.processor = processor
        self.max_length = max_length
        
        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(".jpg")])
        self.task_prompt = "<s_receipt>"

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        image_path = os.path.join(self.image_dir, img_name)
        image = Image.open(image_path).convert("RGB")
        
        label_name = img_name.replace(".jpg", ".json")
        label_path = os.path.join(self.label_dir, label_name)
        
        with open(label_path, "r", encoding="utf-8") as f:
            label_data = json.load(f)

        # í•™ìŠµ ë°©í•´ ìš”ì†Œ 'file' í‚¤ ì œê±°
        if "file" in label_data:
            del label_data["file"]
            
        target_sequence = json.dumps(label_data, ensure_ascii=False)
        
        pixel_values = self.processor(image, return_tensors="pt").pixel_values
        
        input_sequence = self.task_prompt + target_sequence + self.processor.tokenizer.eos_token
        
        labels = self.processor.tokenizer(
            input_sequence,
            add_special_tokens=False,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )["input_ids"]
        
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        return {
            "pixel_values": pixel_values.squeeze(),
            "labels": labels.squeeze()
        }

# ---------------------------------------------------------
# 3. í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
# ---------------------------------------------------------
def train():
    print("ğŸ”¥ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
    
    # 1. í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = DonutProcessor.from_pretrained(MODEL_ID)
    processor.tokenizer.add_tokens(["<s_receipt>", "</s_receipt>"])

    # 2. ëª¨ë¸ ë¡œë“œ
    model = VisionEncoderDecoderModel.from_pretrained(MODEL_ID)
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids("<s_receipt>")
    
    # ì„ë² ë”© í¬ê¸° ì¡°ì •
    model.decoder.resize_token_embeddings(len(processor.tokenizer))
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"âœ… í•™ìŠµ ì¥ì¹˜: {device}")

    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„
    if not os.path.exists(IMAGE_DIR):
        print(f"âŒ ì˜¤ë¥˜: ë°ì´í„° í´ë”({IMAGE_DIR})ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   Kaggle: !cp -r /kaggle/input/... ./dataset ëª…ë ¹ì–´ë¡œ ë°ì´í„°ë¥¼ ë³µì‚¬í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    train_dataset = ReceiptDataset(IMAGE_DIR, LABEL_DIR, processor)
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜: {len(train_dataset)}ì¥")

    # 4. í•™ìŠµ ì¸ì ì„¤ì • (Hub ì—…ë¡œë“œ ì˜µì…˜ ì¶”ê°€)
    training_args = Seq2SeqTrainingArguments(
        output_dir="./result",
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        fp16=True,
        gradient_checkpointing=True,
        logging_steps=10,
        save_strategy="epoch",       # ë§¤ ì—í­ë§ˆë‹¤ ì €ì¥
        save_total_limit=2,          # ìš©ëŸ‰ ê´€ë¦¬
        remove_unused_columns=False,
        report_to="none",
        dataloader_num_workers=2,    # Kaggle/Linuxì—ì„œëŠ” 2~4 ê¶Œì¥
        
        # â˜… Hugging Face Hub ì„¤ì • â˜…
        push_to_hub=PUSH_TO_HUB,
        hub_model_id=HUB_MODEL_ID,
        hub_private_repo=True        # ë¹„ê³µê°œ ì €ì¥ì†Œ ê¶Œì¥
    )

    # 5. íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=processor.feature_extractor, # ì¤‘ìš”: trainerê°€ processor ì €ì¥í•˜ê²Œ í•¨
    )

    # 6. í•™ìŠµ ì‹œì‘
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! (Hub Upload: {PUSH_TO_HUB})")
    trainer.train()

    # 7. ìµœì¢… ì €ì¥ ë° ì—…ë¡œë“œ
    print("ğŸ’¾ ë¡œì»¬ ì €ì¥ ë° Hub ì—…ë¡œë“œ ì¤‘...")
    
    # í”„ë¡œì„¸ì„œë„ ê°™ì´ ì €ì¥í•´ì•¼ ë‚˜ì¤‘ì— ì—ëŸ¬ê°€ ì•ˆ ë‚¨
    processor.save_pretrained("./result")
    
    if PUSH_TO_HUB:
        trainer.push_to_hub()
        print(f"ğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ! Hugging Faceì—ì„œ '{HUB_MODEL_ID}'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! (ì—…ë¡œë“œ ì˜µì…˜ êº¼ì§)")

if __name__ == "__main__":
    train()