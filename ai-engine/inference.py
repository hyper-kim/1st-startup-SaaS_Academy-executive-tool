import torch
import re
import json
from PIL import Image
from transformers import DonutProcessor, VisionEncoderDecoderModel

MODEL_ID = "HYPER-KJY/academy-receipt-model"

# ì „ì—­ ë³€ìˆ˜
model = None
processor = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def load_model_lazy():
    global model, processor
    if model is not None: return

    print(f"ğŸ’¤ Hugging Face Hubì—ì„œ ëª¨ë¸ ë¡œë”© ì¤‘... (ID: {MODEL_ID})")

    try:
        # 1. í”„ë¡œì„¸ì„œ ë¡œë“œ (ì„¤ì • íŒŒì¼ ëˆ„ë½ ëŒ€ë¹„ Fallback)
        try:
            processor = DonutProcessor.from_pretrained(MODEL_ID)
        except OSError:
            print("âš ï¸ í”„ë¡œì„¸ì„œ ì„¤ì •ì´ ì—†ì–´ ê¸°ë³¸ê°’(donut-base)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            processor = DonutProcessor.from_pretrained("naver-clova-ix/donut-base")
            processor.tokenizer.add_tokens(["<s_receipt>", "</s_receipt>"])

        # â˜… [í•µì‹¬ ìˆ˜ì •] ì¶”ë¡ í•  ë•Œë„ í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì€ í•´ìƒë„ë¡œ ê°•ì œ ê³ ì •! â˜…
        # ì´ ì½”ë“œê°€ ì—†ìœ¼ë©´ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ 2ë°° í¬ê²Œ(ì˜ëª») ë´…ë‹ˆë‹¤.
        processor.image_processor.size = {"height": 1280, "width": 960}
        print(f"ğŸ“‰ ì¶”ë¡  ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: {processor.image_processor.size}")

        # 2. ëª¨ë¸ ë¡œë“œ
        model = VisionEncoderDecoderModel.from_pretrained(MODEL_ID)
        model.decoder.resize_token_embeddings(len(processor.tokenizer))
        
        # ëª¨ë¸ ì„¤ì •ì—ë„ ë°˜ì˜ (ì•ˆì „ì¥ì¹˜)
        model.config.encoder.image_size = [1280, 960]
        
        model.to(device)
        model.eval()
        print(f"âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        model = None
        raise e

def run_inference(image_input):
    if model is None:
        try:
            load_model_lazy()
        except Exception as e:
            return {"status": "error", "message": str(e)}

    try:
        if image_input.mode != "RGB":
            image_input = image_input.convert("RGB")

        # ì „ì²˜ë¦¬
        pixel_values = processor(image_input, return_tensors="pt").pixel_values
        pixel_values = pixel_values.to(device)

        task_prompt = "<s_receipt>"
        decoder_input_ids = processor.tokenizer(
            task_prompt, add_special_tokens=False, return_tensors="pt"
        ).input_ids.to(device)

        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                pixel_values,
                decoder_input_ids=decoder_input_ids,
                max_length=768,
                early_stopping=True,
                pad_token_id=processor.tokenizer.pad_token_id,
                eos_token_id=processor.tokenizer.eos_token_id,
                use_cache=True,
                num_beams=1, # ì†ë„ ìœ„í•´ 1ë¡œ ì„¤ì • (í•„ìš” ì‹œ 4)
                bad_words_ids=[[processor.tokenizer.unk_token_id]],
                return_dict_in_generate=True,
            )

        sequence = processor.batch_decode(outputs.sequences)[0]
        sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
        sequence = re.sub(r"<.*?>", "", sequence, count=1).strip()
        
        print(f"ğŸ¤– AI ë¶„ì„ ê²°ê³¼: {sequence}")

        try:
            json_output = processor.token2json(sequence)
            return {"status": "success", "result": json_output}
        except Exception as json_err:
            return {"status": "partial_success", "result": {"text_content": sequence}}

    except Exception as e:
        return {"status": "error", "message": str(e)}