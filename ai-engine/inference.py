# ai-engine/inference.py

import torch
from transformers import DonutProcessor, VisionEncoderDecoderModel
from PIL import Image
import re
import json
import os

# 1. ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ (í•™ìŠµ ê²°ê³¼ë¬¼)
MODEL_PATH = "./models/receipt_model_v1"
IMAGE_PATH = "./dataset/multi_receipt_train/images/multi_receipt_00000.jpg" # í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ

def load_model():
    print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_PATH})")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ì €ì¥ëœ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH).to(device)
    processor = DonutProcessor.from_pretrained(MODEL_PATH)
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {device})")
    return model, processor, device

def run_inference(model, processor, device, image_path):
    # ì´ë¯¸ì§€ ì¤€ë¹„
    image = Image.open(image_path).convert("RGB")
    
    # ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
    pixel_values = processor(image, return_tensors="pt").pixel_values.to(device)
    
    # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (í•™ìŠµ ë•Œ ì¼ë˜ ì‹œì‘ í† í°)
    task_prompt = "<s_receipt>"
    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors="pt").input_ids.to(device)
    
    # ì¶”ë¡  (Generate)
    outputs = model.generate(
        pixel_values,
        decoder_input_ids=decoder_input_ids,
        max_length=768,
        early_stopping=True,
        pad_token_id=processor.tokenizer.pad_token_id,
        eos_token_id=processor.tokenizer.eos_token_id,
        use_cache=True,
        num_beams=1,
        bad_words_ids=[[processor.tokenizer.unk_token_id]],
        return_dict_in_generate=True,
    )
    
    # ê²°ê³¼ ë””ì½”ë”© (í† í° -> í…ìŠ¤íŠ¸)
    sequence = processor.batch_decode(outputs.sequences)[0]
    
    # íŠ¹ìˆ˜ í† í° ì œê±° ë° JSON íŒŒì‹±
    sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(processor.tokenizer.pad_token, "")
    sequence = re.sub(r"<.*?>", "", sequence, count=1).strip()  # ì²« ë²ˆì§¸ <s_receipt> ì œê±°
    
    print(f"\nğŸ§¾ [ì¶”ë¡  ê²°ê³¼ Raw Text]:\n{sequence}\n")
    
    try:
        # JSON ë³€í™˜ ì‹œë„
        result_json = processor.token2json(sequence)
        print(f"âœ¨ [JSON ë³€í™˜ ì„±ê³µ]:")
        print(json.dumps(result_json, ensure_ascii=False, indent=2))
        return result_json
    except Exception as e:
        print(f"âš ï¸ JSON ë³€í™˜ ì‹¤íŒ¨ (í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥): {e}")
        return sequence

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ({MODEL_PATH}) train.pyë¥¼ ë¨¼ì € ì„±ê³µì‹œì¼œì£¼ì„¸ìš”.")
    else:
        model, processor, device = load_model()
        
        # í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(IMAGE_PATH):
            run_inference(model, processor, device, IMAGE_PATH)
        else:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {IMAGE_PATH}")
            print("ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ generate_dataset.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")